{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trier University: Stylometry\n",
    "In this jupyter notebook I will cover the basics of performing stylometric analysis on a large collection of texts.\n",
    "## Using this notebook\n",
    "The code in this notebook is distributed across a few different code blocks. You will need to run them top to bottom, but the actual analysis does not happen until the last block. To run everything, simply click on \"Run All\" in the \"Cell\" menu. I have also provided a plain python file that you can run from the commmand line.\n",
    "\n",
    "## Importing necessary libraries\n",
    "This code block imports the libraries I will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os, sys, platform, json\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Plotting libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager\n",
    "import matplotlib.colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjustable parameters: Analysis\n",
    "The parameters that you might want to adjust for your analysis are contained in the following code block.\n",
    "\n",
    "\n",
    "**ngrams** is an integer that will determine the size of n-gram you use for analysis. 1 will look at single words, 2 will look at two at a time, 3 will look at three at a time, etc. 1 grams work best for most analyses. More than 3 will be slow and often result in very sparse data that is hard to interpret.\n",
    "\n",
    "**commonWords** is an integer that determines how frequent a character must be in the corpus to be considered in the stylometric analysis. 500 will use the 500 most common words across all texts. You can set this to None if you do not want to limit words in this way\n",
    "\n",
    "**limitVocab** is a boolean (True or False). Set to True if you want to specify a specific vocabulary\n",
    "\n",
    "**limitVocabularyFile** is the name of a file that contains the vocabulary you are interested in. The file should have one token per line. This line is only read if limitVocab is set to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of n-grams:\n",
    "ngrams = 1\n",
    "\n",
    "# Limit the number of words to look at\n",
    "commonWords = 500\n",
    "\n",
    "# Set the vocabulary you are interested in\n",
    "limitVocab = False\n",
    "\n",
    "# Vocabulary file\n",
    "limitVocabularyFile = \"vocab.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "By default, this code will tokenize into words based on punctuation and whitespace. This works well with most Romance and Germanic languages However, if you work with a language that does not have whitespace, you will need to adjust the tokenization. You can set it to character based tokenization if you like, or alternatively you can also provide your own tokenizer, if need be.\n",
    "\n",
    "**tokenizationMethod** should be set to either \"word\", \"char\", \"char_wb,\" or \"custom\". \n",
    "\"word\" uses a tokenizer built into sklearn\n",
    "\n",
    "\"char\" will break the text apart into individual characters. \n",
    "\n",
    "\"char_wb\" will break them apart into characters but also respect word boundaries when using an n_gram of greater than 1. If you set to \"custom\" you must provide your own tokenizer! This is best left to more advanced users.\n",
    "\n",
    "**tokenizer** should be a function that splits your text into \"word.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization method:\n",
    "tokenizationMethod = \"word\"\n",
    "\n",
    "# If tokenizationMethod is set to custom, provide your own tokenization function:\n",
    "\n",
    "# this is an extremely basic tokenizer that divides texts according to spaces.  It\n",
    "# is not nearly as sophisticated as the built in tokenizer. \n",
    "# You can also consider using a tokenizer from the Natural Language Toolkit\n",
    "def tokenizer(text):\n",
    "   return text.split(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjustable parameters: Appearance\n",
    "These parameters will help you set the appearance of the plot itself.\n",
    "\n",
    "**labelTypes** is a tuple that specifies the nature of the corpus labeling. Here, the sample corpus files are all  named with the convention author_title_section_genre.txt. Each type of label is one element in this tuple, in the same order they appear in the name\n",
    "\n",
    "**colorValue** this integer specifies which label should be used to generate a color scheme for the plot. 2 points to the 3rd element in the tuple, the siku categorization. There are three different siku categories reflected in the dataset, making this a good option. Here you should pick whichever label your analysis is focused on. More than 8 or so elements, however, will generate colors that are hard to tell apart.\n",
    "\n",
    "**labelValue** this integer specifies which label should be used for labeling the points in the plot. 0 points to the 1st element in the tuple, the title.\n",
    "\n",
    "**pointSize** is an integer that sets how large the points in the plot tare\n",
    "\n",
    "**pointLabels** is a boolean (True or False) that specifies if the points should be labeled.\n",
    "\n",
    "**plotLoadings** is a boolean that specifies if the vocabulary should be drawn on the plot (which will aid in interpretation). The further a term is from the center of the plot, the more it is influencing texts in a given direction.\n",
    "\n",
    "**hidePoints** is a boolean that specifies of the points should be drawn. Set to False to see the loadings better.\n",
    "\n",
    "**outputDimensions** is a tuple that sets the width and height of the output plot in inches. The inner values can be either integers or floats.\n",
    "\n",
    "**outputFile** contains the name of the outputfile, where the plot will be saved. The file extension will determine file type. png, pdf, jpg, tif, and others are all valid selections. On Macs, because of an oddity of the plotting library, pdfs will be very large. You can fix this by opening the file with adobe illustrator (or another similar program) and then saving a copy. This is because the entire font is embedded in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Types of labels for documents in the corpus\n",
    "# This must match your metadata naming scheme!\n",
    "labelTypes = ('author', 'title', 'section', 'genre') # tuple with strings\n",
    "\n",
    "# Some of these labels will set the color used to differentiate the points in the plot.\n",
    "# The label at this index is used to set Color:\n",
    "colorValue = 3 # Index of label to use for color (integer). Here 3 points to \"genre\"\n",
    "\n",
    "# Index of label to use for plot labels (if points are labeled)\n",
    "labelValue = 0 # Index of label to use for labels (integer). Here 0 points to \"title\"\n",
    "\n",
    "# Point size (integer)\n",
    "pointSize = 8\n",
    "\n",
    "# Show point labels (add labels for each text):\n",
    "pointLabels = False # True or False\n",
    "\n",
    "# Plot loadings (write the characters tot he plot)\n",
    "plotLoadings = False # True or False\n",
    "\n",
    "# Hide points (useful for seeing loadings better):\n",
    "hidePoints = False # True or False\n",
    "\n",
    "# Output file info (dimensions are in inches (width, height)):\n",
    "outputDimensions = (10, 7.5) # Tuple of integers or floats\n",
    "\n",
    "# Output file extension determines output type. Save as a pdf if you want to edit in illustator\n",
    "# PDF Output on mac is very large, but just opening and saving a copy in illustrator will fix this\n",
    "outputFile = \"myfigure.png\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjustable Parameters: no need to change\n",
    "This parameters can be adjusted, but you may as well leave them as they are.\n",
    "\n",
    "**pcaComponents** is an integer that sets how many principal components should be calculated. We are only using two in this analysis, but as you work more with these plots, you can consider setting this higher (but you will also have to adjust later parts of the script to make them do anything). The maximum this can be is the number of variables (here the 500 words) minus one (so 499 in this case).\n",
    "\n",
    "**corpusFolder** is the name of the folder that holds the corpus files. Just leave this as \"corpus\" if you put your files in a folder called \"corpus\".\n",
    "\n",
    "**removeItemsFile** is a string that points to words (tokens) that you want to remove from consideration. Each token to be removed should be on a line in the specified file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many components?\n",
    "pcaComponents = 2 # Only useful for digging even deeper in the data\n",
    "\n",
    "# Input folder\n",
    "corpusFolder = \"corpus\"\n",
    "\n",
    "# Tokens to remove from consideration\n",
    "removeItemsFile = \"remove.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nothing beyond here needs editing!!\n",
    "The comments in the code itself explain what is happening. If you run the script from a terminal, it will open a new window with your plot. It will look like the code keeps running until you close this window. This is an interactive explorer you can use to study the plot itself. Here it will just insert the figure after the codeblock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading, cleaning, and tokenizing\n",
      "322 of 322 processed\n",
      "Vectorizing\n",
      "Performing PCA\n",
      "Setting plot info\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-98a6e205e7a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0mcolorDictionaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0muniqueLabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muniqueLabelValues\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     \u001b[0mcolorpalette\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolor_palette\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"husl\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniqueLabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_hex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m     \u001b[0mcolorDictionaries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniqueLabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolorpalette\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sns' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x540 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "####################\n",
    "# Type Enforcement #\n",
    "####################\n",
    "\n",
    "# This section enforces the input values for all the adjustable variables. This\n",
    "# is to make sure the script isn't run incorrectly.\n",
    "\n",
    "# function to check values\n",
    "def valueChecker(varname, typeofobj, value):\n",
    "    if type(typeofobj) == type:\n",
    "        if typeofobj == bool and type(value) != typeofobj:\n",
    "            print(f\"{varname} must be a {typeofobj} (True or False). Please fix to run script.\")\n",
    "            sys.exit()\n",
    "        if type(value) != typeofobj:\n",
    "            print(f\"{varname} must be {typeofobj}. Please fix to run script.\")\n",
    "            sys.exec_info()\n",
    "            sys.exit() \n",
    "    elif type(typeofobj) == tuple:\n",
    "        if type(value) != typeofobj[0] and type(value) != typeofobj[1]:\n",
    "            print(f\"{varname} must be {typeofobj[0]} or {typeofobj[1]}. Please fix to run script.\")\n",
    "            sys.exit() \n",
    "\n",
    "# check values\n",
    "valueChecker('ngrams', int, ngrams)\n",
    "if commonWords:\n",
    "    valueChecker('commonWords', int, commonWords)\n",
    "else:\n",
    "    pass\n",
    "valueChecker('limitVocab', bool, limitVocab)\n",
    "valueChecker('colorValue', int, colorValue)\n",
    "valueChecker('labelValue', int, labelValue)\n",
    "valueChecker('pointSize', int, pointSize)\n",
    "valueChecker('pointLabels', bool, pointLabels)\n",
    "valueChecker('plotLoadings', bool, plotLoadings)\n",
    "valueChecker('hidePoints', bool, hidePoints)\n",
    "valueChecker('outputFile', str, outputFile)\n",
    "valueChecker('pcaComponents', int, pcaComponents)\n",
    "valueChecker('corpusFolder', str, corpusFolder)\n",
    "valueChecker('removeItemsFile', str, removeItemsFile)\n",
    "\n",
    "# check tuples and internal values\n",
    "if type(labelTypes) != tuple:\n",
    "    print('labelTypes must be a tuple. Please fix to run script.')\n",
    "    sys.exit()\n",
    "else:\n",
    "    for lab in labelTypes:\n",
    "        valueChecker('labelType item', str, lab)\n",
    "\n",
    "if type(outputDimensions) != tuple:\n",
    "    print(f\"outputDimensions must be {tuple}. Please fix to run the script\")\n",
    "else:\n",
    "    for d in outputDimensions:\n",
    "        valueChecker(\"outerDimension value\", (float, int), d)\n",
    "\n",
    "# check tokenization methods:\n",
    "if tokenizationMethod not in {\"word\", \"char\", \"char_wb\", \"custom\"}:\n",
    "    print(\"tokenizationMethod must be a string set to 'word', 'char', 'char_wb', or 'custom'.\")\n",
    "        \n",
    "# Load in external files\n",
    "try:\n",
    "    if removeItemsFile:\n",
    "        removeItems = []\n",
    "        with open(removeItemsFile, \"r\", encoding='utf8') as rf:\n",
    "            removeItems = [item.strip() for item in rf.read().split(\"\\n\") if item != \"\"]\n",
    "    else:\n",
    "        removeItems = []\n",
    "except FileNotFoundError:\n",
    "    print(f\"No file named {removeItemsFile} found. Please check filename or create the file.\")\n",
    "    sys.exit()\n",
    "\n",
    "if limitVocab == True:\n",
    "    valueChecker('limitVocabularyFile', str, limitVocabularyFile)\n",
    "    try:\n",
    "        limitVocabulary = [] \n",
    "        with open(limitVocabularyFile, \"r\", encoding='utf8') as rf:\n",
    "            limitVocabulary = [item.strip() for item in rf.read().split(\"\\n\") if item != \"\"]\n",
    "        if commonWords:\n",
    "            print(f\"You are limiting analysis to the {commonWords} most common words but also using a set vocabulary.\")\n",
    "            print(\"If you want to avoid unexpected behavior, set commonWords to None when limiting vocab.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"No file named {limitVocabularyFile} found. Please check filename or create the file\")\n",
    "        print(\"Defaulting to no limit on the vocabulary\")       \n",
    "        limitVocabulary = None\n",
    "else:\n",
    "    limitVocabulary = None\n",
    "\n",
    "# Ensure corpus folder exists\n",
    "if not os.path.isdir(corpusFolder):\n",
    "    print(f\"Could not find the corpus folder '{corpusFolder}'. Please double check.\")\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "########################\n",
    "# Function definitions #\n",
    "########################\n",
    "\n",
    "# Function to clean the text. Remove desired characters and white space.\n",
    "def clean(text, removeitems):\n",
    "    for item in removeitems:\n",
    "        text = text.replace(item, \"\")\n",
    "    text = re.sub(\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "##############\n",
    "# Load Texts #\n",
    "##############\n",
    "\n",
    "print(\"Loading, cleaning, and tokenizing\")\n",
    "# Go through each document in the corpus folder and save info to lists\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "for root, dirs, files in os.walk(corpusFolder):\n",
    "    for i, f in enumerate(files):\n",
    "        if f not in {'.DS_Store', \".txt\"}:\n",
    "            # add the labels to the label list\n",
    "            labels.append(f[:-4].split(\"_\"))\n",
    "\n",
    "            # Open the text, clean it, and tokenize it\n",
    "            with open(os.path.join(root,f),\"r\", encoding='utf8', errors='ignore') as rf:\n",
    "                texts.append(clean(rf.read(), removeItems))\n",
    "            \n",
    "            if i == len(files) - 1:\n",
    "                print(f\"\\r{i+1} of {len(files)} processed\", end='\\n', flush=True)\n",
    "            else:\n",
    "                print(f\"\\r{i+1} of {len(files)} processed\", end='', flush=True)\n",
    "\n",
    "####################\n",
    "# Perform Analysis #\n",
    "####################\n",
    "\n",
    "print(\"Vectorizing\")\n",
    "if tokenizationMethod != \"custom\":\n",
    "    countVectorizer = TfidfVectorizer(max_features=commonWords, use_idf=False, vocabulary=limitVocabulary,  ngram_range=(ngrams, ngrams), analyzer=tokenizationMethod)\n",
    "else:\n",
    "    countVectorizer = TfidfVectorizer(max_features=commonWords, use_idf=False, vocabulary=limitVocabulary,  ngram_range=(ngrams, ngrams), analyzer=\"word\", tokenizer=tokenizer)\n",
    "    \n",
    "countMatrix = countVectorizer.fit_transform(texts)\n",
    "countMatrix = countMatrix.toarray()\n",
    "\n",
    "print(\"Performing PCA\")\n",
    "# Lets perform PCA on the countMatrix:\n",
    "pca = PCA(n_components=pcaComponents)\n",
    "myPCA = pca.fit_transform(countMatrix)\n",
    "\n",
    "\n",
    "##############\n",
    "# Plot Setup #\n",
    "##############\n",
    "\n",
    "print(\"Setting plot info\")\n",
    "# set the plot size\n",
    "plt.figure(figsize=outputDimensions)\n",
    "\n",
    "# find all the unique values for each of the label types\n",
    "uniqueLabelValues = [set() for i in range(len(labelTypes))]\n",
    "for labelList in labels:\n",
    "    for i, label in enumerate(labelList):\n",
    "        uniqueLabelValues[i].add(label)\n",
    "\n",
    "# create color dictionaries for all labels\n",
    "colorDictionaries = []\n",
    "for uniqueLabels in uniqueLabelValues:\n",
    "    colorpalette = sns.color_palette(\"husl\",len(uniqueLabels)).as_hex()\n",
    "    colorDictionaries.append(dict(zip(uniqueLabels,colorpalette)))\n",
    "\n",
    "# Now we need the Unique Labels\n",
    "uniqueColorLabels = list(uniqueLabelValues[colorValue])\n",
    "# Let's get a number for each class\n",
    "numberForClass = [i for i in range(len(uniqueColorLabels))]\n",
    "\n",
    "# Make a dictionary! This is new sytax for us! It just makes a dictionary where\n",
    "# the keys are the unique years and the values are found in numberForClass\n",
    "labelForClassNumber = dict(zip(uniqueColorLabels,numberForClass))\n",
    "\n",
    "# Let's make a new representation for each document that is just these integers\n",
    "# and it needs to be a numpy array\n",
    "textClass = np.array([labelForClassNumber[lab[colorValue]] for lab in labels])\n",
    "\n",
    "\n",
    "# Make a list of the colors\n",
    "colors = [colorDictionaries[colorValue][lab] for lab in uniqueColorLabels]\n",
    "\n",
    "if hidePoints:\n",
    "    pointSize = 0\n",
    "\n",
    "###################\n",
    "# Create the plot #\n",
    "###################\n",
    "\n",
    "print(\"Plotting texts\")\n",
    "for col, classNumber, lab in zip(colors, numberForClass, uniqueColorLabels):\n",
    "    plt.scatter(myPCA[textClass==classNumber,0],myPCA[textClass==classNumber,1],label=lab,c=col, s=pointSize)\n",
    "\n",
    "# Let's label individual points so we know WHICH document they are\n",
    "if pointLabels:\n",
    "    print(\"Adding Labels\")\n",
    "    for lab, datapoint in zip(labels, myPCA):\n",
    "        plt.annotate(str(lab[labelValue]),xy=datapoint)\n",
    "\n",
    "# Let's graph component loadings\n",
    "vocabulary = countVectorizer.get_feature_names()\n",
    "loadings = pca.components_\n",
    "if plotLoadings:\n",
    "    print(\"Rendering Loadings\")    \n",
    "    for i, word in enumerate(vocabulary):\n",
    "        plt.annotate(word, xy=(loadings[0, i], loadings[1,i]))\n",
    "    \n",
    "\n",
    "# Let's add a legend! matplotlib will make this for us based on the data we \n",
    "# gave the scatter function.\n",
    "plt.legend()\n",
    "plt.savefig(outputFile)\n",
    "\n",
    "\n",
    "############################################\n",
    "# Output data for JavaScript Visualization #\n",
    "############################################\n",
    "\n",
    "data = []\n",
    "for datapoint in myPCA:\n",
    "    pcDict = {}\n",
    "    for i, dp in enumerate(datapoint):\n",
    "        pcDict[f\"PC{str(i + 1)}\"] = dp\n",
    "    data.append(pcDict)\n",
    "\n",
    "jsLoadings = []\n",
    "for i, word in enumerate(vocabulary):\n",
    "    temploading = {}\n",
    "    for j,dp in enumerate(loadings):\n",
    "        temploading[f\"PC{str(j+1)}\"] = dp[i]\n",
    "    jsLoadings.append([word, temploading])\n",
    "\n",
    "colorDictionaryList = []\n",
    "for cd in colorDictionaries:\n",
    "    cdlist = [v for v in cd.values()]\n",
    "    colorDictionaryList.append(cdlist)\n",
    "\n",
    "colorstrings = json.dumps(colorDictionaryList)\n",
    "labelstrings = json.dumps(labels)\n",
    "valuetypes = json.dumps([k for k in data[0].keys()])\n",
    "datastrings = json.dumps(data)\n",
    "\n",
    "limitedlabeltypes = []\n",
    "for i, t in enumerate(labelTypes):\n",
    "    if len(uniqueLabelValues[i]) <= 20:\n",
    "        limitedlabeltypes.append(t)\n",
    "\n",
    "cattypestrings = json.dumps(limitedlabeltypes)\n",
    "loadingstrings = json.dumps(jsLoadings)\n",
    "stringlist = [f\"var colorDictionaries = {colorstrings};\", f\"var labels = {labelstrings};\",\n",
    "            f\"var data = {datastrings};\", f\"var categoryTypes = {list(labelTypes)};\", \n",
    "            f\"var loadings = {jsLoadings};\", f\"var valueTypes = {valuetypes};\",\n",
    "            f\"var limitedCategories = {limitedlabeltypes};\",\n",
    "            f\"var activecatnum = {colorValue};\", f\"var activelabelnum = {labelValue};\",\n",
    "            f\"var explainedvariance = [{round(pca.explained_variance_[0],3)},{round(pca.explained_variance_[1],3)}]\"]\n",
    "\n",
    "\n",
    "with open(\"data.js\", \"w\", encoding=\"utf8\") as wf:\n",
    "    wf.write(\"\\n\".join(stringlist))\n",
    "\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
